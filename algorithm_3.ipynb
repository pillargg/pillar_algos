{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pillaralgos.helpers import data_handler as dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "\n",
    "1. Give users who have older accounts more weight in calculating the optimal timestamps\n",
    "2. Give users who used more words in the twitch chat more weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_stats(dataframe):\n",
    "    '''\n",
    "    Returns the user who sent the most messages, most emoticons, and who changed their username the most\n",
    "    '''\n",
    "    temp_df = dataframe.groupby(\"name\").count().reset_index()\n",
    "    temp_df['username_chg'] = temp_df['_id'] - temp_df['display_name'] # if mismatch in count, then its probs changed username\n",
    "    \n",
    "    stats = {}\n",
    "    for col in ['_id','emoticons', 'username_chg']:\n",
    "        sort = temp_df.sort_values(col, ascending=False)[['name',col]].iloc[0]\n",
    "        stat = sort[col]\n",
    "        _id = sort['name']\n",
    "        \n",
    "        if col == '_id':\n",
    "            col = 'num_messages'\n",
    "        stats[col] = [_id, stat]\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find number of words for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_words = pd.DataFrame(columns = ['display_name','_id','num_words', 'num_emoji'])\n",
    "for _id in big_df['_id'].unique():\n",
    "    temp_df = big_df[big_df['_id'] == _id]\n",
    "    words = temp_df['body'].str.split(' ')\n",
    "    emoji = temp_df['emoticons'].apply(lambda x: 0 if type(x) == float else len(x))\n",
    "    num_emoji = emoji.sum()\n",
    "    num_words = words.apply(lambda x: len(x))\n",
    "    \n",
    "    sum_words = num_words.sum()\n",
    "    id_words = id_words.append({\n",
    "        'display_name':temp_df['display_name'].iloc[0],\n",
    "        '_id':_id,\n",
    "        'num_words':sum_words,\n",
    "        'num_emoji':num_emoji\n",
    "    }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_words = id_words.astype({'_id':int,'num_words':int,'num_emoji':int})\n",
    "id_words['only_words'] = id_words['num_words'] - id_words['num_emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_words.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_words = id_words.sort_values('only_words',ascending=False).reset_index(drop=True).reset_index()\n",
    "id_words = id_words.rename({'index':'rank'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create user weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight = \"num words / num words of biggest spammer\"\n",
    "id_words['weight'] = id_words['only_words'] / id_words['only_words'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_stamp, chunk_list = dh.get_chunks(big_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dict = {} # store chunks of the top 10 users\n",
    "for user in id_words.head(10)['_id']:\n",
    "    user_dict[user] = []\n",
    "\n",
    "for user in user_dict.keys():\n",
    "    for chunk in chunk_list:\n",
    "        if sum(chunk['_id'] == user) > 5:\n",
    "            user_dict[user].append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_chunks = []\n",
    "for user, value in user_dict.items():\n",
    "    if len(value) > 0:\n",
    "        for chunk in value:\n",
    "            chunk['num_top_user_appears'] = chunk['_id']==user\n",
    "            top_chunks.append(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`top_dict` now contains all chunks where at least one of the top 10 users appears more than 5 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_formatter(list_chunk, goal):\n",
    "    '''\n",
    "    Creates a new df `results` that contains the total number of words in the dataframe, the time\n",
    "    the time the dataframe started and ended\n",
    "    input\n",
    "    -----\n",
    "    list_chunk: list\n",
    "        List of pd.DataFrame\n",
    "    goal: str\n",
    "        Col name that has calculated results (ex: num_words, chat_rate, etc.)\n",
    "    output\n",
    "    ------\n",
    "    results: pd.DataFrame\n",
    "        Dataframe with `start`, `end`, `num_words` columns\n",
    "    '''\n",
    "    chunk_list = []\n",
    "    time_start_list = []\n",
    "    time_end_list = []\n",
    "    goal_list = []\n",
    "    \n",
    "    for chunk in list_chunk:\n",
    "        time_start_list.append(chunk.iloc[0, 0])  # assume created_at col is first\n",
    "        time_end_list.append(chunk.iloc[-1, 0])  # assume created_at col is first\n",
    "        goal_list.append(chunk[goal].sum())\n",
    "\n",
    "    results = pd.DataFrame({\n",
    "        'start':time_start_list,\n",
    "        'end':time_end_list,\n",
    "        goal:goal_list\n",
    "    })\n",
    "    results = results.sort_values(goal, ascending=False).drop_duplicates()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results_formatter(top_chunks, goal='num_top_user_appears')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_results = dh.results_jsonified(results, first_stamp, results_col = 'num_top_user_appears')\n",
    "dh.save_json(json_results,'algo3_top_user_appears')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# This script finds the number of words/emojis/both depending on `goal` variable,\n",
    "# isolates to each `min_` timestamp, and sorts the resulting df by largest number\n",
    "########\n",
    "import pandas as pd\n",
    "from helpers import data_handler as dh\n",
    "\n",
    "def thalamus(big_df, min_, goal, min_words):\n",
    "    \n",
    "    id_words = id_words_counter(big_df)\n",
    "    first_stamp, chunk_list = dh.get_chunks(big_df)\n",
    "    top_chunks = new_chunk_list(id_words, chunk_list, min_words = 5)\n",
    "    results = results_formatter(top_chunks, goal='num_top_user_appears')\n",
    "    \n",
    "    return results, first_stamp\n",
    "    \n",
    "def results_formatter(list_chunk, goal):\n",
    "    '''\n",
    "    Creates a new df `results` that contains the total number of words in the dataframe, the time\n",
    "    the time the dataframe started and ended\n",
    "    input\n",
    "    -----\n",
    "    list_chunk: list\n",
    "        List of pd.DataFrame\n",
    "    goal: str\n",
    "        Col name that has calculated results (ex: num_words, chat_rate, etc.)\n",
    "    output\n",
    "    ------\n",
    "    results: pd.DataFrame\n",
    "        Dataframe with `start`, `end`, `num_words` columns\n",
    "    '''\n",
    "    chunk_list = []\n",
    "    time_start_list = []\n",
    "    time_end_list = []\n",
    "    goal_list = []\n",
    "    \n",
    "    for chunk in list_chunk:\n",
    "        time_start_list.append(chunk.iloc[0, 0])  # assume created_at col is first\n",
    "        time_end_list.append(chunk.iloc[-1, 0])  # assume created_at col is first\n",
    "        goal_list.append(chunk[goal].sum())\n",
    "\n",
    "    results = pd.DataFrame({\n",
    "        'start':time_start_list,\n",
    "        'end':time_end_list,\n",
    "        goal:goal_list\n",
    "    })\n",
    "    results = results.sort_values(goal, ascending=False).drop_duplicates()\n",
    "    return results\n",
    "    \n",
    "def new_chunk_list(id_words, chunk_list, min_words):\n",
    "    '''\n",
    "    Creates a new list of chunks, containing only chunks where top\n",
    "    users sent more than `min_words` words\n",
    "    '''\n",
    "    user_dict = {} # store chunks of the top 10 users\n",
    "    for user in id_words.head(10)['_id']:\n",
    "        user_dict[user] = []\n",
    "    for user in user_dict.keys():\n",
    "        for chunk in chunk_list:\n",
    "            chunk['_id'] = chunk['_id'].astype(int)\n",
    "            if sum(chunk['_id'] == user) > min_words:\n",
    "                user_dict[user].append(chunk)\n",
    "    top_chunks = []\n",
    "    for user, value in user_dict.items():\n",
    "        if len(value) > 0:\n",
    "            for chunk in value:\n",
    "                chunk['num_top_user_appears'] = chunk['_id']==user\n",
    "                top_chunks.append(chunk)\n",
    "    return top_chunks\n",
    "\n",
    "def id_words_counter(big_df):\n",
    "    '''\n",
    "    Returns a dataframe with all user IDs and the number of words/emojis/combined\n",
    "    they each sent, sorted by top senders\n",
    "    '''\n",
    "    id_words = pd.DataFrame(columns = ['display_name','_id','num_words', 'num_emoji'])\n",
    "    \n",
    "    for _id in big_df['_id'].unique():\n",
    "        temp_df = big_df[big_df['_id'] == _id]\n",
    "        words = temp_df['body'].str.split(' ')\n",
    "        emoji = temp_df['emoticons'].apply(lambda x: 0 if type(x) == float else len(x))\n",
    "        num_emoji = emoji.sum()\n",
    "        num_words = words.apply(lambda x: len(x))\n",
    "\n",
    "        sum_words = num_words.sum()\n",
    "        id_words = id_words.append({\n",
    "            'display_name':temp_df['display_name'].iloc[0],\n",
    "            '_id':_id,\n",
    "            'num_words':sum_words,\n",
    "            'num_emoji':num_emoji\n",
    "        }, ignore_index=True)\n",
    "    id_words = id_words.astype({'_id':int,'num_words':int,'num_emoji':int})\n",
    "    id_words['only_words'] = id_words['num_words'] - id_words['num_emoji']\n",
    "    id_words = id_words.sort_values('only_words',ascending=False).reset_index(drop=True).reset_index()\n",
    "    id_words = id_words.rename({'index':'rank'},axis=1)\n",
    "    # Weight = \"num words / num words of biggest spammer\"\n",
    "    id_words['weight'] = id_words['only_words'] / id_words['only_words'].max()\n",
    "    \n",
    "    return id_words\n",
    "    \n",
    "def run(data, min_, min_words):\n",
    "    data = pd.DataFrame.from_records(data)\n",
    "    big_df = dh.organize_twitch_chat(data) # fetch appropriate data\n",
    "    results, first_stamp = thalamus(big_df, min_, min_words = 5, goal='num_top_user_appears') \n",
    "    \n",
    "    json_results = dh.results_jsonified(results, first_stamp, results_col='num_top_user_appears')\n",
    "    dh.save_json(json_results, f\"algo3.0_top_user_appears\")\n",
    "    \n",
    "    return json_results, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_results, results = run(data,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pillar Env",
   "language": "python",
   "name": "pillar_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
