{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Pillar Algos Background Algorithms Datasets Current Goal Long Term Goal Testing Locally Usage Background Pillar is creating an innovative way to automatically select and splice clips from Twitch videos for streamers. This repo is focusing on the algorithm aspect. Three main algorithms are being tested. Algorithms Algorithm 1 : Find the best moments in clips based on where the most users participated. Most is defined as the ratio of unique users during a 2 min section to unique users for the entire session. Algorithm 2 Find the best moments in clips based on when rate of messages per user peaked. This involves answering the question \"at which 2 min segment do the most users send the most messages?\". If users X, Y, and Z all send 60% of their messages at timestamp range delta, then that timestamp might qualify as a \"best moment\" NOTE : Currently answers the question \"at which 2 min segment do users send the most messages fastest\" Algorithm 3 (WIP) Weigh each user by their chat rate, account age, etc. Heavier users predicted to chat more often at \"best moment\" timestamps STATUS : current weight determined by ( num_words_of_user / num_words_of_top_user ) Algorithm 3.5 Finds the best moments in clips based on most number of words/emojis/both used in chat Datasets: Preliminary data prelim_df : 545 rows representing one 3 hour 35 minute 26 second twitch stream chat of Hearthstone by LiiHS Used to create initial json import and resulting df clean/merge function organize_twitch_chat Big data big_df : 2409 rows representing one 7 hour 37 minute, 0 second twitch stream chat of Hearthstone by LiiHS Used to create all algorithms Current Goal To create one overarching algorithm that will find the most \"interesting\" clips in a twitch VOD. This will be created through the following steps: 1. Creation of various algorithms that isolate min_ (2 by default) minute chunks. The basic workflow: 1. Create variable (ex: num_words , for number of words in the body of a chat message) 1. Group df by min_ chunks, then average/sum/etc num_words for each min_ chunks 1. Sort new df by num_words , from highest \"value\" to lowest \"value\" 1. Return this new df as json ( example ) 1. Users rate clips provided by each algorithm 2. Useless algorithms thrown away 3. Rest of the algorithms merged into one overarching algorithm, with weights distributed based on user ratings Long Term Goal New objective measure : community created clips ( ccc ) for a given VOD id with start/end timestamps for each clip Assumption : ccc are interesting and can be used to create a narrative for each VOD. We can test this by cross referencing with posts to /r/livestreamfails upvotes/comments Hypothesis : if we can predict where ccc would be created, those are potentially good clips to show the user Short term test : Create a model to predict where ccc would be created using variables such as word count, chat rate, emoji usage, chat semantic analysis. We can do this by finding timestamps of ccc and correlating them with chat stats Medium term test : Use top 100 streamers as training data. What similarities do their ccc and reddit most upvoted of that VOD share? (chat rate etc) Get the transcript for these top 100 Get the top 100's YT posted 15-30min story content for the 8 hour VOD Get the transcript for that story content Semantic analysis and correlations, etc. Long term test : what percentage of clips do our streamers actually end up using Testing Locally There are two methods to test locally, with one working on just Unix-based OSes, and one working on all OSes. Testing with Containers will work on all OSes with Docker installed. Testing with Containers This is the recommended method for testing and should work on all operating systems. First, install Docker and act . This is a tool that allows the running of GitHub Actions CI locally, which is how we will run the tests on your local machine. After both are installed, you have to generate a GitHub Token. See [here] on how to do that. The token should be stored in a file called .secrets in your working directory (this file is in the .gitignore so it will not be committed). Here is what the file should look like: GITHUB_TOKEN=<your token here> After, you can run the tests with the following: act Here is an example of what output should look like: Log [Run Unit Tests/test] \ud83d\ude80 Start image=catthehacker/ubuntu:act-20.04 [Run Unit Tests/test] \ud83d\udc33 docker run image=catthehacker/ubuntu:act-20.04 platform= entrypoint=[\"/usr/bin/tail\" \"-f\" \"/dev/null\"] cmd=[] [Run Unit Tests/test] \u2b50 Run Checkout Code [Run Unit Tests/test] \u2601 git clone 'https://github.com/actions/checkout' # ref=v2 [Run Unit Tests/test] \ud83d\udc33 docker cp src=C:\\Users\\chizz\\.cache\\act/actions-checkout@v2/ dst=/mnt/c/Users/chizz/Documents/pillar/pillar_algos/_actions/actions-checkout@v2/ [Run Unit Tests/test] \u2753 ::save-state name=isPost,::true [Run Unit Tests/test] \ud83d\udcac ::debug::GITHUB_WORKSPACE = '/mnt/c/Users/chizz/Documents/pillar/pillar_algos' [Run Unit Tests/test] \ud83d\udcac ::debug::qualified repository = 'pillargg/pillar_algos' [Run Unit Tests/test] \ud83d\udcac ::debug::ref = 'refs/heads/ci-cd' [Run Unit Tests/test] \ud83d\udcac ::debug::commit = '74c766153947d78164c3dc4fde3cffac2efe3630' [Run Unit Tests/test] \ud83d\udcac ::debug::clean = true [Run Unit Tests/test] \ud83d\udcac ::debug::fetch depth = 1 [Run Unit Tests/test] \ud83d\udcac ::debug::lfs = false [Run Unit Tests/test] \ud83d\udcac ::debug::submodules = false [Run Unit Tests/test] \ud83d\udcac ::debug::recursive submodules = false [Run Unit Tests/test] \u2753 ::add-matcher::/mnt/c/Users/chizz/Documents/pillar/pillar_algos/_actions/actions-checkout@v2/dist/problem-matcher.json | Syncing repository: pillargg/pillar_algos [Run Unit Tests/test] \u2753 ::group::Getting Git version info | Working directory is '/mnt/c/Users/chizz/Documents/pillar/pillar_algos' [Run Unit Tests/test] \ud83d\udcac ::debug::Getting git version | [command]/usr/bin/git version | git version 2.25.1 [Run Unit Tests/test] \ud83d\udcac ::debug::Set git useragent to: git/2.25.1 (github-actions-checkout) [Run Unit Tests/test] \u2753 ::endgroup:: | Deleting the contents of '/mnt/c/Users/chizz/Documents/pillar/pillar_algos' [Run Unit Tests/test] \u2753 ::save-state name=repositoryPath,::/mnt/c/Users/chizz/Documents/pillar/pillar_algos [Run Unit Tests/test] \u2753 ::group::Initializing the repository | [command]/usr/bin/git init /mnt/c/Users/chizz/Documents/pillar/pillar_algos | Initialized empty Git repository in /mnt/c/Users/chizz/Documents/pillar/pillar_algos/.git/ | [command]/usr/bin/git remote add origin https://github.com/pillargg/pillar_algos [Run Unit Tests/test] \u2753 ::endgroup:: [Run Unit Tests/test] \u2753 ::group::Disabling automatic garbage collection | [command]/usr/bin/git config --local gc.auto 0 [Run Unit Tests/test] \u2753 ::endgroup:: [Run Unit Tests/test] \u2699 ::add-mask::eC1hY2Nlc3MtdG9rZW46Z2hwX0d0ZDR4TUlDcVp6cGZoNkI5T1BRdGJOdXpvOGdBNTNiTmd6Vg== [Run Unit Tests/test] \u2753 ::group::Setting up auth | [command]/usr/bin/git config --local --name-only --get-regexp core\\.sshCommand | [command]/usr/bin/git submodule foreach --recursive git config --local --name-only --get-regexp 'core\\.sshCommand' && git config --local --unset-all 'core.sshCommand' || : | [command]/usr/bin/git config --local --name-only --get-regexp http\\.https\\:\\/\\/github\\.com\\/\\.extraheader | [command]/usr/bin/git submodule foreach --recursive git config --local --name-only --get-regexp 'http\\.https\\:\\/\\/github\\.com\\/\\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || : | [command]/usr/bin/git config --local http.https://github.com/.extraheader AUTHORIZATION: basic *** [Run Unit Tests/test] \u2753 ::endgroup:: [Run Unit Tests/test] \u2753 ::group::Fetching the repository | [command]/usr/bin/git -c protocol.version=2 fetch --no-tags --prune --progress --no-recurse-submodules --depth=1 origin +74c766153947d78164c3dc4fde3cffac2efe3630:refs/remotes/origin/ci-cd | remote: Enumerating objects: 56, done. remote: Counting objects: 100% (56/56), done. remote: Compressing objects: 100% (48/48), done. | remote: Total 56 (delta 3), reused 33 (delta 1), pack-reused 0 | From https://github.com/pillargg/pillar_algos | * [new ref] 74c766153947d78164c3dc4fde3cffac2efe3630 -> origin/ci-cd [Run Unit Tests/test] \u2753 ::endgroup:: [Run Unit Tests/test] \u2753 ::group::Determining the checkout info [Run Unit Tests/test] \u2753 ::endgroup:: [Run Unit Tests/test] \u2753 ::group::Checking out the ref | [command]/usr/bin/git checkout --progress --force -B ci-cd refs/remotes/origin/ci-cd | Switched to a new branch 'ci-cd' | Branch 'ci-cd' set up to track remote branch 'ci-cd' from 'origin'. [Run Unit Tests/test] \u2753 ::endgroup:: | [command]/usr/bin/git log -1 --format='%H' | '74c766153947d78164c3dc4fde3cffac2efe3630' [Run Unit Tests/test] \u2753 ::remove-matcher owner=checkout-git,:: [Run Unit Tests/test] \u2705 Success - Checkout Code [Run Unit Tests/test] \u2b50 Run Setup Python 3.8 [Run Unit Tests/test] \u2601 git clone 'https://github.com/actions/setup-python' # ref=v2 [Run Unit Tests/test] \ud83d\udc33 docker cp src=C:\\Users\\chizz\\.cache\\act/actions-setup-python@v2/ dst=/mnt/c/Users/chizz/Documents/pillar/pillar_algos/_actions/actions-setup-python@v2/ [Run Unit Tests/test] \ud83d\udcac ::debug::Semantic version spec of 3.8 is 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::isExplicit: [Run Unit Tests/test] \ud83d\udcac ::debug::explicit? false [Run Unit Tests/test] \ud83d\udcac ::debug::evaluating 0 versions [Run Unit Tests/test] \ud83d\udcac ::debug::match not found | Version 3.8 was not found in the local cache [Run Unit Tests/test] \ud83d\udcac ::debug::set auth [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.10.0-beta.1 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.10.0-alpha.7 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.10.0-alpha.6 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.10.0-alpha.5 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.10.0-alpha.4 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.10.0-alpha.3 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.10.0-alpha.2 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.10.0-alpha.1 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.5 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.4 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.3 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.2 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.2-rc.1 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.1 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.1-rc.1 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.0 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.0-rc.2 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.0-rc.1 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.0-beta.5 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.0-beta.4 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.8.10 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::x64===x64 && darwin===linux [Run Unit Tests/test] \ud83d\udcac ::debug::x64===x64 && linux===linux [Run Unit Tests/test] \ud83d\udcac ::debug::x64===x64 && linux===linux [Run Unit Tests/test] \ud83d\udcac ::debug::x64===x64 && linux===linux [Run Unit Tests/test] \ud83d\udcac ::debug::matched 3.8.10 | Version 3.8 is available for downloading | Download from \"https://github.com/actions/python-versions/releases/download/3.8.10-107001/python-3.8.10-linux-20.04-x64.tar.gz\" [Run Unit Tests/test] \ud83d\udcac ::debug::Downloading https://github.com/actions/python-versions/releases/download/3.8.10-107001/python-3.8.10-linux-20.04-x64.tar.gz [Run Unit Tests/test] \ud83d\udcac ::debug::Destination /tmp/bfc2b5ad-62f1-4101-8131-960763558bae [Run Unit Tests/test] \ud83d\udcac ::debug::set auth [Run Unit Tests/test] \ud83d\udcac ::debug::download complete | Extract downloaded archive [Run Unit Tests/test] \ud83d\udcac ::debug::Checking tar --version [Run Unit Tests/test] \ud83d\udcac ::debug::tar (GNU tar) 1.30%0ACopyright (C) 2017 Free Software Foundation, Inc.%0ALicense GPLv3+: GNU GPL version 3 or later <https://gnu.org/licenses/gpl.html>.%0AThis is free software: you are free to change and redistribute it.%0AThere is NO WARRANTY, to the extent permitted by law.%0A%0AWritten by John Gilmore and Jay Fenlason. | [command]/usr/bin/tar xz --warning=no-unknown-keyword -C /tmp/b50560b8-c29b-4368-9961-8116845c95a2 -f /tmp/bfc2b5ad-62f1-4101-8131-960763558bae | Execute installation script | Check if Python hostedtoolcache folder exist... | Creating Python hostedtoolcache folder... | Create Python 3.8.10 folder | Copy Python binaries to hostedtoolcache folder | Create additional symlinks (Required for the UsePythonVersion Azure Pipelines task and the setup-python GitHub Action) | Upgrading PIP... | Looking in links: /tmp/tmp46b32jk4 | Requirement already satisfied: setuptools in /opt/hostedtoolcache/Python/3.8.10/x64/lib/python3.8/site-packages (56.0.0) | Requirement already satisfied: pip in /opt/hostedtoolcache/Python/3.8.10/x64/lib/python3.8/site-packages (21.1.1) [Run Unit Tests/test] \u2757 ::error::WARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv | Collecting pip | Downloading pip-21.1.1-py3-none-any.whl (1.5 MB) | Installing collected packages: pip | Successfully installed pip-21.1.1 [Run Unit Tests/test] \u2757 ::error::WARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv | Create complete file [Run Unit Tests/test] \ud83d\udcac ::debug::isExplicit: [Run Unit Tests/test] \ud83d\udcac ::debug::explicit? false [Run Unit Tests/test] \ud83d\udcac ::debug::isExplicit: 3.8.10 [Run Unit Tests/test] \ud83d\udcac ::debug::explicit? true [Run Unit Tests/test] \ud83d\udcac ::debug::evaluating 1 versions [Run Unit Tests/test] \ud83d\udcac ::debug::matched: 3.8.10 [Run Unit Tests/test] \ud83d\udcac ::debug::checking cache: /opt/hostedtoolcache/Python/3.8.10/x64 [Run Unit Tests/test] \ud83d\udcac ::debug::Found tool in cache Python 3.8.10 x64 [Run Unit Tests/test] \u2699 ::set-output:: python-version=3.8.10 | Successfully setup CPython (3.8.10) [Run Unit Tests/test] \u2753 ##[add-matcher]/mnt/c/Users/chizz/Documents/pillar/pillar_algos/_actions/actions-setup-python@v2/.github/python.json [Run Unit Tests/test] \u2705 Success - Setup Python 3.8 [Run Unit Tests/test] \u2b50 Run Install Dependencies | Collecting pandas | Downloading pandas-1.2.4-cp38-cp38-manylinux1_x86_64.whl (9.7 MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.7 MB 2.9 MB/s | Collecting numpy | Downloading numpy-1.20.3-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.4 MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15.4 MB 62.5 MB/s | Collecting pre-commit | Downloading pre_commit-2.12.1-py2.py3-none-any.whl (189 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 189 kB 55.3 MB/s | Collecting pillaralgos | Downloading pillaralgos-1.0.18-py3-none-any.whl (18 kB) | Collecting python-dateutil>=2.7.3 | Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 227 kB 37.9 MB/s | Collecting pytz>=2017.3 | Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 510 kB 54.0 MB/s | Collecting six>=1.5 | Downloading six-1.16.0-py2.py3-none-any.whl (11 kB) | Collecting toml | Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB) | Collecting virtualenv>=20.0.8 | Downloading virtualenv-20.4.6-py2.py3-none-any.whl (7.2 MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.2 MB 28.6 MB/s | Collecting identify>=1.0.0 | Downloading identify-2.2.4-py2.py3-none-any.whl (98 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 98 kB 14.4 MB/s | Collecting nodeenv>=0.11.1 | Downloading nodeenv-1.6.0-py2.py3-none-any.whl (21 kB) | Collecting cfgv>=2.0.0 | Downloading cfgv-3.2.0-py2.py3-none-any.whl (7.3 kB) | Collecting pyyaml>=5.1 | Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 662 kB 42.5 MB/s | Collecting filelock<4,>=3.0.0 | Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB) | Collecting appdirs<2,>=1.4.3 | Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB) | Collecting distlib<1,>=0.3.1 | Downloading distlib-0.3.1-py2.py3-none-any.whl (335 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 335 kB 48.7 MB/s | Installing collected packages: six, pytz, python-dateutil, numpy, filelock, distlib, appdirs, virtualenv, toml, pyyaml, pandas, nodeenv, identify, cfgv, pre-commit, pillaralgos | Successfully installed appdirs-1.4.4 cfgv-3.2.0 distlib-0.3.1 filelock-3.0.12 identify-2.2.4 nodeenv-1.6.0 numpy-1.20.3 pandas-1.2.4 pillaralgos-1.0.18 pre-commit-2.12.1 python-dateutil-2.8.1 pytz-2021.1 pyyaml-5.4.1 six-1.16.0 toml-0.10.2 virtualenv-20.4.6 | WARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv | Collecting pytest | Downloading pytest-6.2.4-py3-none-any.whl (280 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 280 kB 2.8 MB/s | Collecting py>=1.8.2 | Downloading py-1.10.0-py2.py3-none-any.whl (97 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 97 kB 4.3 MB/s | Collecting attrs>=19.2.0 | Downloading attrs-21.2.0-py2.py3-none-any.whl (53 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 53 kB 2.6 MB/s | Requirement already satisfied: toml in /opt/hostedtoolcache/Python/3.8.10/x64/lib/python3.8/site-packages (from pytest) (0.10.2) | Collecting iniconfig | Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB) | Collecting packaging | Downloading packaging-20.9-py2.py3-none-any.whl (40 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40 kB 4.0 MB/s | Collecting pluggy<1.0.0a1,>=0.12 | Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB) | Collecting pyparsing>=2.0.2 | Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 67 kB 4.0 MB/s | Installing collected packages: pyparsing, py, pluggy, packaging, iniconfig, attrs, pytest | Successfully installed attrs-21.2.0 iniconfig-1.1.1 packaging-20.9 pluggy-0.13.1 py-1.10.0 pyparsing-2.4.7 pytest-6.2.4 | WARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv [Run Unit Tests/test] \u2705 Success - Install Dependencies [Run Unit Tests/test] \u2b50 Run Run Tests | ============================= test session starts ============================== | platform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 | rootdir: /mnt/c/Users/chizz/Documents/pillar/pillar_algos/prod/test collected 17 items | | test_algos.py ........ [ 47%] | test_brain.py .. [ 58%] | test_helpers.py ....... [100%] | | ============================== slowest durations =============================== | 30.90s call test_algos.py::test_algo2_diffs | 17.57s call test_brain.py::test_length | 17.45s call test_brain.py::test_brain | 14.16s call test_algos.py::test_algo30_diffs | 11.40s call test_algos.py::test_algo2 | 9.11s call test_algos.py::test_algo35_diffs | 4.88s call test_algos.py::test_algo3_0 | 4.32s call test_algos.py::test_algo3_5 | 2.48s call test_helpers.py::test_emoji_getter | 2.21s call test_algos.py::test_algo1_diffs | 1.49s setup test_helpers.py::test_emoji_getter | 0.97s call test_algos.py::test_algo1 | 0.08s call test_helpers.py::test_organize_twitch_chat_lg | 0.08s call test_helpers.py::test_organize_twitch_chat_dtypes | 0.08s call test_helpers.py::test_organize_twitch_chat_cols | 0.08s setup test_brain.py::test_brain | 0.07s setup test_brain.py::test_length | 0.06s setup test_helpers.py::test_organize_twitch_chat_cols | 0.06s setup test_helpers.py::test_organize_twitch_chat_dtypes | 0.06s setup test_algos.py::test_algo30_diffs | 0.06s setup test_algos.py::test_algo2 | 0.06s setup test_algos.py::test_algo3_5 | 0.05s setup test_algos.py::test_algo1 | 0.05s setup test_helpers.py::test_organize_twitch_chat_lg | 0.05s setup test_algos.py::test_algo3_0 | 0.04s setup test_algos.py::test_algo35_diffs | 0.04s setup test_algos.py::test_algo1_diffs | 0.04s setup test_algos.py::test_algo2_diffs | | (23 durations < 0.005s hidden. Use -vv to show these durations.) | ======================== 17 passed in 118.31s (0:01:58) ======================== [Run Unit Tests/test] \u2705 Success - Run Tests Testing without Containers You need Python 3.8 or newer installed. Run the following in the project directory. pip install -r requirement.txt pip install pytest cd prod/test pytest --duration=0 Usage from pillaralgos import OPTIONS # replace options with an algo OPTIONS.run(data) # see algo docstring for descriptions","title":"Pillar Algos"},{"location":"#pillar-algos","text":"Background Algorithms Datasets Current Goal Long Term Goal Testing Locally Usage","title":"Pillar Algos"},{"location":"#background","text":"Pillar is creating an innovative way to automatically select and splice clips from Twitch videos for streamers. This repo is focusing on the algorithm aspect. Three main algorithms are being tested.","title":"Background"},{"location":"#algorithms","text":"Algorithm 1 : Find the best moments in clips based on where the most users participated. Most is defined as the ratio of unique users during a 2 min section to unique users for the entire session. Algorithm 2 Find the best moments in clips based on when rate of messages per user peaked. This involves answering the question \"at which 2 min segment do the most users send the most messages?\". If users X, Y, and Z all send 60% of their messages at timestamp range delta, then that timestamp might qualify as a \"best moment\" NOTE : Currently answers the question \"at which 2 min segment do users send the most messages fastest\" Algorithm 3 (WIP) Weigh each user by their chat rate, account age, etc. Heavier users predicted to chat more often at \"best moment\" timestamps STATUS : current weight determined by ( num_words_of_user / num_words_of_top_user ) Algorithm 3.5 Finds the best moments in clips based on most number of words/emojis/both used in chat","title":"Algorithms"},{"location":"#datasets","text":"Preliminary data prelim_df : 545 rows representing one 3 hour 35 minute 26 second twitch stream chat of Hearthstone by LiiHS Used to create initial json import and resulting df clean/merge function organize_twitch_chat Big data big_df : 2409 rows representing one 7 hour 37 minute, 0 second twitch stream chat of Hearthstone by LiiHS Used to create all algorithms","title":"Datasets:"},{"location":"#current-goal","text":"To create one overarching algorithm that will find the most \"interesting\" clips in a twitch VOD. This will be created through the following steps: 1. Creation of various algorithms that isolate min_ (2 by default) minute chunks. The basic workflow: 1. Create variable (ex: num_words , for number of words in the body of a chat message) 1. Group df by min_ chunks, then average/sum/etc num_words for each min_ chunks 1. Sort new df by num_words , from highest \"value\" to lowest \"value\" 1. Return this new df as json ( example ) 1. Users rate clips provided by each algorithm 2. Useless algorithms thrown away 3. Rest of the algorithms merged into one overarching algorithm, with weights distributed based on user ratings","title":"Current Goal"},{"location":"#long-term-goal","text":"New objective measure : community created clips ( ccc ) for a given VOD id with start/end timestamps for each clip Assumption : ccc are interesting and can be used to create a narrative for each VOD. We can test this by cross referencing with posts to /r/livestreamfails upvotes/comments Hypothesis : if we can predict where ccc would be created, those are potentially good clips to show the user Short term test : Create a model to predict where ccc would be created using variables such as word count, chat rate, emoji usage, chat semantic analysis. We can do this by finding timestamps of ccc and correlating them with chat stats Medium term test : Use top 100 streamers as training data. What similarities do their ccc and reddit most upvoted of that VOD share? (chat rate etc) Get the transcript for these top 100 Get the top 100's YT posted 15-30min story content for the 8 hour VOD Get the transcript for that story content Semantic analysis and correlations, etc. Long term test : what percentage of clips do our streamers actually end up using","title":"Long Term Goal"},{"location":"#testing-locally","text":"There are two methods to test locally, with one working on just Unix-based OSes, and one working on all OSes. Testing with Containers will work on all OSes with Docker installed.","title":"Testing Locally"},{"location":"#testing-with-containers","text":"This is the recommended method for testing and should work on all operating systems. First, install Docker and act . This is a tool that allows the running of GitHub Actions CI locally, which is how we will run the tests on your local machine. After both are installed, you have to generate a GitHub Token. See [here] on how to do that. The token should be stored in a file called .secrets in your working directory (this file is in the .gitignore so it will not be committed). Here is what the file should look like: GITHUB_TOKEN=<your token here> After, you can run the tests with the following: act Here is an example of what output should look like: Log [Run Unit Tests/test] \ud83d\ude80 Start image=catthehacker/ubuntu:act-20.04 [Run Unit Tests/test] \ud83d\udc33 docker run image=catthehacker/ubuntu:act-20.04 platform= entrypoint=[\"/usr/bin/tail\" \"-f\" \"/dev/null\"] cmd=[] [Run Unit Tests/test] \u2b50 Run Checkout Code [Run Unit Tests/test] \u2601 git clone 'https://github.com/actions/checkout' # ref=v2 [Run Unit Tests/test] \ud83d\udc33 docker cp src=C:\\Users\\chizz\\.cache\\act/actions-checkout@v2/ dst=/mnt/c/Users/chizz/Documents/pillar/pillar_algos/_actions/actions-checkout@v2/ [Run Unit Tests/test] \u2753 ::save-state name=isPost,::true [Run Unit Tests/test] \ud83d\udcac ::debug::GITHUB_WORKSPACE = '/mnt/c/Users/chizz/Documents/pillar/pillar_algos' [Run Unit Tests/test] \ud83d\udcac ::debug::qualified repository = 'pillargg/pillar_algos' [Run Unit Tests/test] \ud83d\udcac ::debug::ref = 'refs/heads/ci-cd' [Run Unit Tests/test] \ud83d\udcac ::debug::commit = '74c766153947d78164c3dc4fde3cffac2efe3630' [Run Unit Tests/test] \ud83d\udcac ::debug::clean = true [Run Unit Tests/test] \ud83d\udcac ::debug::fetch depth = 1 [Run Unit Tests/test] \ud83d\udcac ::debug::lfs = false [Run Unit Tests/test] \ud83d\udcac ::debug::submodules = false [Run Unit Tests/test] \ud83d\udcac ::debug::recursive submodules = false [Run Unit Tests/test] \u2753 ::add-matcher::/mnt/c/Users/chizz/Documents/pillar/pillar_algos/_actions/actions-checkout@v2/dist/problem-matcher.json | Syncing repository: pillargg/pillar_algos [Run Unit Tests/test] \u2753 ::group::Getting Git version info | Working directory is '/mnt/c/Users/chizz/Documents/pillar/pillar_algos' [Run Unit Tests/test] \ud83d\udcac ::debug::Getting git version | [command]/usr/bin/git version | git version 2.25.1 [Run Unit Tests/test] \ud83d\udcac ::debug::Set git useragent to: git/2.25.1 (github-actions-checkout) [Run Unit Tests/test] \u2753 ::endgroup:: | Deleting the contents of '/mnt/c/Users/chizz/Documents/pillar/pillar_algos' [Run Unit Tests/test] \u2753 ::save-state name=repositoryPath,::/mnt/c/Users/chizz/Documents/pillar/pillar_algos [Run Unit Tests/test] \u2753 ::group::Initializing the repository | [command]/usr/bin/git init /mnt/c/Users/chizz/Documents/pillar/pillar_algos | Initialized empty Git repository in /mnt/c/Users/chizz/Documents/pillar/pillar_algos/.git/ | [command]/usr/bin/git remote add origin https://github.com/pillargg/pillar_algos [Run Unit Tests/test] \u2753 ::endgroup:: [Run Unit Tests/test] \u2753 ::group::Disabling automatic garbage collection | [command]/usr/bin/git config --local gc.auto 0 [Run Unit Tests/test] \u2753 ::endgroup:: [Run Unit Tests/test] \u2699 ::add-mask::eC1hY2Nlc3MtdG9rZW46Z2hwX0d0ZDR4TUlDcVp6cGZoNkI5T1BRdGJOdXpvOGdBNTNiTmd6Vg== [Run Unit Tests/test] \u2753 ::group::Setting up auth | [command]/usr/bin/git config --local --name-only --get-regexp core\\.sshCommand | [command]/usr/bin/git submodule foreach --recursive git config --local --name-only --get-regexp 'core\\.sshCommand' && git config --local --unset-all 'core.sshCommand' || : | [command]/usr/bin/git config --local --name-only --get-regexp http\\.https\\:\\/\\/github\\.com\\/\\.extraheader | [command]/usr/bin/git submodule foreach --recursive git config --local --name-only --get-regexp 'http\\.https\\:\\/\\/github\\.com\\/\\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || : | [command]/usr/bin/git config --local http.https://github.com/.extraheader AUTHORIZATION: basic *** [Run Unit Tests/test] \u2753 ::endgroup:: [Run Unit Tests/test] \u2753 ::group::Fetching the repository | [command]/usr/bin/git -c protocol.version=2 fetch --no-tags --prune --progress --no-recurse-submodules --depth=1 origin +74c766153947d78164c3dc4fde3cffac2efe3630:refs/remotes/origin/ci-cd | remote: Enumerating objects: 56, done. remote: Counting objects: 100% (56/56), done. remote: Compressing objects: 100% (48/48), done. | remote: Total 56 (delta 3), reused 33 (delta 1), pack-reused 0 | From https://github.com/pillargg/pillar_algos | * [new ref] 74c766153947d78164c3dc4fde3cffac2efe3630 -> origin/ci-cd [Run Unit Tests/test] \u2753 ::endgroup:: [Run Unit Tests/test] \u2753 ::group::Determining the checkout info [Run Unit Tests/test] \u2753 ::endgroup:: [Run Unit Tests/test] \u2753 ::group::Checking out the ref | [command]/usr/bin/git checkout --progress --force -B ci-cd refs/remotes/origin/ci-cd | Switched to a new branch 'ci-cd' | Branch 'ci-cd' set up to track remote branch 'ci-cd' from 'origin'. [Run Unit Tests/test] \u2753 ::endgroup:: | [command]/usr/bin/git log -1 --format='%H' | '74c766153947d78164c3dc4fde3cffac2efe3630' [Run Unit Tests/test] \u2753 ::remove-matcher owner=checkout-git,:: [Run Unit Tests/test] \u2705 Success - Checkout Code [Run Unit Tests/test] \u2b50 Run Setup Python 3.8 [Run Unit Tests/test] \u2601 git clone 'https://github.com/actions/setup-python' # ref=v2 [Run Unit Tests/test] \ud83d\udc33 docker cp src=C:\\Users\\chizz\\.cache\\act/actions-setup-python@v2/ dst=/mnt/c/Users/chizz/Documents/pillar/pillar_algos/_actions/actions-setup-python@v2/ [Run Unit Tests/test] \ud83d\udcac ::debug::Semantic version spec of 3.8 is 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::isExplicit: [Run Unit Tests/test] \ud83d\udcac ::debug::explicit? false [Run Unit Tests/test] \ud83d\udcac ::debug::evaluating 0 versions [Run Unit Tests/test] \ud83d\udcac ::debug::match not found | Version 3.8 was not found in the local cache [Run Unit Tests/test] \ud83d\udcac ::debug::set auth [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.10.0-beta.1 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.10.0-alpha.7 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.10.0-alpha.6 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.10.0-alpha.5 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.10.0-alpha.4 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.10.0-alpha.3 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.10.0-alpha.2 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.10.0-alpha.1 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.5 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.4 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.3 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.2 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.2-rc.1 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.1 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.1-rc.1 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.0 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.0-rc.2 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.0-rc.1 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.0-beta.5 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.9.0-beta.4 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::check 3.8.10 satisfies 3.8 [Run Unit Tests/test] \ud83d\udcac ::debug::x64===x64 && darwin===linux [Run Unit Tests/test] \ud83d\udcac ::debug::x64===x64 && linux===linux [Run Unit Tests/test] \ud83d\udcac ::debug::x64===x64 && linux===linux [Run Unit Tests/test] \ud83d\udcac ::debug::x64===x64 && linux===linux [Run Unit Tests/test] \ud83d\udcac ::debug::matched 3.8.10 | Version 3.8 is available for downloading | Download from \"https://github.com/actions/python-versions/releases/download/3.8.10-107001/python-3.8.10-linux-20.04-x64.tar.gz\" [Run Unit Tests/test] \ud83d\udcac ::debug::Downloading https://github.com/actions/python-versions/releases/download/3.8.10-107001/python-3.8.10-linux-20.04-x64.tar.gz [Run Unit Tests/test] \ud83d\udcac ::debug::Destination /tmp/bfc2b5ad-62f1-4101-8131-960763558bae [Run Unit Tests/test] \ud83d\udcac ::debug::set auth [Run Unit Tests/test] \ud83d\udcac ::debug::download complete | Extract downloaded archive [Run Unit Tests/test] \ud83d\udcac ::debug::Checking tar --version [Run Unit Tests/test] \ud83d\udcac ::debug::tar (GNU tar) 1.30%0ACopyright (C) 2017 Free Software Foundation, Inc.%0ALicense GPLv3+: GNU GPL version 3 or later <https://gnu.org/licenses/gpl.html>.%0AThis is free software: you are free to change and redistribute it.%0AThere is NO WARRANTY, to the extent permitted by law.%0A%0AWritten by John Gilmore and Jay Fenlason. | [command]/usr/bin/tar xz --warning=no-unknown-keyword -C /tmp/b50560b8-c29b-4368-9961-8116845c95a2 -f /tmp/bfc2b5ad-62f1-4101-8131-960763558bae | Execute installation script | Check if Python hostedtoolcache folder exist... | Creating Python hostedtoolcache folder... | Create Python 3.8.10 folder | Copy Python binaries to hostedtoolcache folder | Create additional symlinks (Required for the UsePythonVersion Azure Pipelines task and the setup-python GitHub Action) | Upgrading PIP... | Looking in links: /tmp/tmp46b32jk4 | Requirement already satisfied: setuptools in /opt/hostedtoolcache/Python/3.8.10/x64/lib/python3.8/site-packages (56.0.0) | Requirement already satisfied: pip in /opt/hostedtoolcache/Python/3.8.10/x64/lib/python3.8/site-packages (21.1.1) [Run Unit Tests/test] \u2757 ::error::WARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv | Collecting pip | Downloading pip-21.1.1-py3-none-any.whl (1.5 MB) | Installing collected packages: pip | Successfully installed pip-21.1.1 [Run Unit Tests/test] \u2757 ::error::WARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv | Create complete file [Run Unit Tests/test] \ud83d\udcac ::debug::isExplicit: [Run Unit Tests/test] \ud83d\udcac ::debug::explicit? false [Run Unit Tests/test] \ud83d\udcac ::debug::isExplicit: 3.8.10 [Run Unit Tests/test] \ud83d\udcac ::debug::explicit? true [Run Unit Tests/test] \ud83d\udcac ::debug::evaluating 1 versions [Run Unit Tests/test] \ud83d\udcac ::debug::matched: 3.8.10 [Run Unit Tests/test] \ud83d\udcac ::debug::checking cache: /opt/hostedtoolcache/Python/3.8.10/x64 [Run Unit Tests/test] \ud83d\udcac ::debug::Found tool in cache Python 3.8.10 x64 [Run Unit Tests/test] \u2699 ::set-output:: python-version=3.8.10 | Successfully setup CPython (3.8.10) [Run Unit Tests/test] \u2753 ##[add-matcher]/mnt/c/Users/chizz/Documents/pillar/pillar_algos/_actions/actions-setup-python@v2/.github/python.json [Run Unit Tests/test] \u2705 Success - Setup Python 3.8 [Run Unit Tests/test] \u2b50 Run Install Dependencies | Collecting pandas | Downloading pandas-1.2.4-cp38-cp38-manylinux1_x86_64.whl (9.7 MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.7 MB 2.9 MB/s | Collecting numpy | Downloading numpy-1.20.3-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.4 MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15.4 MB 62.5 MB/s | Collecting pre-commit | Downloading pre_commit-2.12.1-py2.py3-none-any.whl (189 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 189 kB 55.3 MB/s | Collecting pillaralgos | Downloading pillaralgos-1.0.18-py3-none-any.whl (18 kB) | Collecting python-dateutil>=2.7.3 | Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 227 kB 37.9 MB/s | Collecting pytz>=2017.3 | Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 510 kB 54.0 MB/s | Collecting six>=1.5 | Downloading six-1.16.0-py2.py3-none-any.whl (11 kB) | Collecting toml | Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB) | Collecting virtualenv>=20.0.8 | Downloading virtualenv-20.4.6-py2.py3-none-any.whl (7.2 MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.2 MB 28.6 MB/s | Collecting identify>=1.0.0 | Downloading identify-2.2.4-py2.py3-none-any.whl (98 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 98 kB 14.4 MB/s | Collecting nodeenv>=0.11.1 | Downloading nodeenv-1.6.0-py2.py3-none-any.whl (21 kB) | Collecting cfgv>=2.0.0 | Downloading cfgv-3.2.0-py2.py3-none-any.whl (7.3 kB) | Collecting pyyaml>=5.1 | Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 662 kB 42.5 MB/s | Collecting filelock<4,>=3.0.0 | Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB) | Collecting appdirs<2,>=1.4.3 | Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB) | Collecting distlib<1,>=0.3.1 | Downloading distlib-0.3.1-py2.py3-none-any.whl (335 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 335 kB 48.7 MB/s | Installing collected packages: six, pytz, python-dateutil, numpy, filelock, distlib, appdirs, virtualenv, toml, pyyaml, pandas, nodeenv, identify, cfgv, pre-commit, pillaralgos | Successfully installed appdirs-1.4.4 cfgv-3.2.0 distlib-0.3.1 filelock-3.0.12 identify-2.2.4 nodeenv-1.6.0 numpy-1.20.3 pandas-1.2.4 pillaralgos-1.0.18 pre-commit-2.12.1 python-dateutil-2.8.1 pytz-2021.1 pyyaml-5.4.1 six-1.16.0 toml-0.10.2 virtualenv-20.4.6 | WARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv | Collecting pytest | Downloading pytest-6.2.4-py3-none-any.whl (280 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 280 kB 2.8 MB/s | Collecting py>=1.8.2 | Downloading py-1.10.0-py2.py3-none-any.whl (97 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 97 kB 4.3 MB/s | Collecting attrs>=19.2.0 | Downloading attrs-21.2.0-py2.py3-none-any.whl (53 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 53 kB 2.6 MB/s | Requirement already satisfied: toml in /opt/hostedtoolcache/Python/3.8.10/x64/lib/python3.8/site-packages (from pytest) (0.10.2) | Collecting iniconfig | Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB) | Collecting packaging | Downloading packaging-20.9-py2.py3-none-any.whl (40 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40 kB 4.0 MB/s | Collecting pluggy<1.0.0a1,>=0.12 | Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB) | Collecting pyparsing>=2.0.2 | Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 67 kB 4.0 MB/s | Installing collected packages: pyparsing, py, pluggy, packaging, iniconfig, attrs, pytest | Successfully installed attrs-21.2.0 iniconfig-1.1.1 packaging-20.9 pluggy-0.13.1 py-1.10.0 pyparsing-2.4.7 pytest-6.2.4 | WARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv [Run Unit Tests/test] \u2705 Success - Install Dependencies [Run Unit Tests/test] \u2b50 Run Run Tests | ============================= test session starts ============================== | platform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 | rootdir: /mnt/c/Users/chizz/Documents/pillar/pillar_algos/prod/test collected 17 items | | test_algos.py ........ [ 47%] | test_brain.py .. [ 58%] | test_helpers.py ....... [100%] | | ============================== slowest durations =============================== | 30.90s call test_algos.py::test_algo2_diffs | 17.57s call test_brain.py::test_length | 17.45s call test_brain.py::test_brain | 14.16s call test_algos.py::test_algo30_diffs | 11.40s call test_algos.py::test_algo2 | 9.11s call test_algos.py::test_algo35_diffs | 4.88s call test_algos.py::test_algo3_0 | 4.32s call test_algos.py::test_algo3_5 | 2.48s call test_helpers.py::test_emoji_getter | 2.21s call test_algos.py::test_algo1_diffs | 1.49s setup test_helpers.py::test_emoji_getter | 0.97s call test_algos.py::test_algo1 | 0.08s call test_helpers.py::test_organize_twitch_chat_lg | 0.08s call test_helpers.py::test_organize_twitch_chat_dtypes | 0.08s call test_helpers.py::test_organize_twitch_chat_cols | 0.08s setup test_brain.py::test_brain | 0.07s setup test_brain.py::test_length | 0.06s setup test_helpers.py::test_organize_twitch_chat_cols | 0.06s setup test_helpers.py::test_organize_twitch_chat_dtypes | 0.06s setup test_algos.py::test_algo30_diffs | 0.06s setup test_algos.py::test_algo2 | 0.06s setup test_algos.py::test_algo3_5 | 0.05s setup test_algos.py::test_algo1 | 0.05s setup test_helpers.py::test_organize_twitch_chat_lg | 0.05s setup test_algos.py::test_algo3_0 | 0.04s setup test_algos.py::test_algo35_diffs | 0.04s setup test_algos.py::test_algo1_diffs | 0.04s setup test_algos.py::test_algo2_diffs | | (23 durations < 0.005s hidden. Use -vv to show these durations.) | ======================== 17 passed in 118.31s (0:01:58) ======================== [Run Unit Tests/test] \u2705 Success - Run Tests","title":"Testing with Containers"},{"location":"#testing-without-containers","text":"You need Python 3.8 or newer installed. Run the following in the project directory. pip install -r requirement.txt pip install pytest cd prod/test pytest --duration=0","title":"Testing without Containers"},{"location":"#usage","text":"from pillaralgos import OPTIONS # replace options with an algo OPTIONS.run(data) # see algo docstring for descriptions","title":"Usage"},{"location":"Algorithm_1/","text":"Sorts the final results by perc_rel_unique . Calculated as \"number of chatters at timestamp\"/\"number of chatters in that one hour\" HOW TO: algo1.run(data, min_=2, limit=10, sort_by='rel', save_json = False) hour_iterator ( big_df , limit , min_ = 2 , sort_by = 'rel' ) Pushes all dfs in a list through the xminChats function, returns a dataframe of results Input big_df: pd.DataFrame Df of the entire twitch session. This is the one that was split by dfSplitter class min_: int How long a timestamp range should be sort_by: str Whether to sort values by `abs` or `rel` unique chatters. perc_uniques ( chunk_list , min_ , total_uniques , big_unique ) Finds the percent unique chatters for each dataframe in the list. Dataframes assumed to be split using xminChats.find_rest. run ( data , min_ = 2 , limit = 10 , sort_by = 'rel' , save_json = False ) Runs algo1 to sort timestamps by the relative percentage of chatters by default. Input data: list List of dictionaries of data from Twitch chat min_: int Approximate number of minutes each clip should be limit: int Number of rows/dictionaries/timestamps to return sort_by: str 'rel': \"number of chatters at timestamp\"/\"number of chatters at that hour\" 'abs': \"number of chatters at timestamp\"/\"total number of chatters in stream\" save_json: bool True if want to save results as json to exports folder","title":"Algorithm 1"},{"location":"Algorithm_1/#prod.pillaralgos.algo1.hour_iterator","text":"Pushes all dfs in a list through the xminChats function, returns a dataframe of results","title":"hour_iterator()"},{"location":"Algorithm_1/#prod.pillaralgos.algo1.hour_iterator--input","text":"big_df: pd.DataFrame Df of the entire twitch session. This is the one that was split by dfSplitter class min_: int How long a timestamp range should be sort_by: str Whether to sort values by `abs` or `rel` unique chatters.","title":"Input"},{"location":"Algorithm_1/#prod.pillaralgos.algo1.perc_uniques","text":"Finds the percent unique chatters for each dataframe in the list. Dataframes assumed to be split using xminChats.find_rest.","title":"perc_uniques()"},{"location":"Algorithm_1/#prod.pillaralgos.algo1.run","text":"Runs algo1 to sort timestamps by the relative percentage of chatters by default.","title":"run()"},{"location":"Algorithm_1/#prod.pillaralgos.algo1.run--input","text":"data: list List of dictionaries of data from Twitch chat min_: int Approximate number of minutes each clip should be limit: int Number of rows/dictionaries/timestamps to return sort_by: str 'rel': \"number of chatters at timestamp\"/\"number of chatters at that hour\" 'abs': \"number of chatters at timestamp\"/\"total number of chatters in stream\" save_json: bool True if want to save results as json to exports folder","title":"Input"},{"location":"Algorithm_2/","text":"This script finds the mean chat_rate per unique user per min_ min chunk, isolates to each min_ timestamp, and sorts the resulting df by largest number HOW TO algo2.run(data, min_=2, limit=10, save_json = False) rate_finder ( dataframe , x = 2 ) Finds the rate of messages sent per X minutes for each user in the dataframe (assumed to be a chunk). NOTE : if only 1 timestamp in chunk dataframe, assumes the chunk is exactly X minutes before the next chunk in the entire twitch chat stream run ( data , min_ = 2 , limit = 10 , save_json = False ) Runs algo2 to find the mean chat_rate per unique user per min_ chunk, takes the means for each chunk, and then sorts by the highest mean rate. Input data: list List of dictionaries of data from Twitch chat min_: int Approximate number of minutes each clip should be limit: int Number of rows/dictionaries/timestamps to return save_json: bool True if want to save results as json to exports folder Output json_results: list List of dictionaries in json format, ordered from predicted best to worst candidates. Ex: [{start:TIMESTAMP_INT, end:TIMESTAMP_INT}] thalamus ( dataframe , min_ ) Formats data for rate_finder(), gets chunk_list to pass through rate_finder","title":"Algorithm 2"},{"location":"Algorithm_2/#prod.pillaralgos.algo2.rate_finder","text":"Finds the rate of messages sent per X minutes for each user in the dataframe (assumed to be a chunk). NOTE : if only 1 timestamp in chunk dataframe, assumes the chunk is exactly X minutes before the next chunk in the entire twitch chat stream","title":"rate_finder()"},{"location":"Algorithm_2/#prod.pillaralgos.algo2.run","text":"Runs algo2 to find the mean chat_rate per unique user per min_ chunk, takes the means for each chunk, and then sorts by the highest mean rate.","title":"run()"},{"location":"Algorithm_2/#prod.pillaralgos.algo2.run--input","text":"data: list List of dictionaries of data from Twitch chat min_: int Approximate number of minutes each clip should be limit: int Number of rows/dictionaries/timestamps to return save_json: bool True if want to save results as json to exports folder","title":"Input"},{"location":"Algorithm_2/#prod.pillaralgos.algo2.run--output","text":"json_results: list List of dictionaries in json format, ordered from predicted best to worst candidates. Ex: [{start:TIMESTAMP_INT, end:TIMESTAMP_INT}]","title":"Output"},{"location":"Algorithm_2/#prod.pillaralgos.algo2.thalamus","text":"Formats data for rate_finder(), gets chunk_list to pass through rate_finder","title":"thalamus()"},{"location":"Algorithm_3.0/","text":"This script finds the top 10 active users, timestamps where they participated, filtered by at least min_words number of words sent by the user per stamp HOW TO algo3_0.run(data, min_=2, limit=10, min_words=5, save_json = False) id_words_counter ( big_df ) Returns a dataframe with all user IDs and the number of words/emojis/combined they each sent, sorted by top senders new_chunk_list ( id_words , chunk_list , min_words ) Creates a new list of chunks, containing only chunks where top users sent more than min_words words results_formatter ( list_chunk , goal ) Creates a new df results that contains the total number of words in the dataframe, the time the time the dataframe started and ended Input list_chunk: list List of pd.DataFrame goal: str Col name that has calculated results (ex: num_words, chat_rate, etc.) Output results: pd.DataFrame Dataframe with `start`, `end`, `num_words` columns run ( data , min_ = 2 , limit = 10 , min_words = 5 , save_json = False ) Runs algo3_0 to extract only those chunks where the top 10 users participated. - Top users are defined as \"sent the most words in the entire twitch stream\". - Once top users are identified, only those chunks are returned where top users sent at least min_words number of words. Input data: list List of dictionaries of data from Twitch chat min_: int Approximate number of minutes each clip should be limit: int Number of rows/dictionaries/timestamps to return min_words:int When filtering chunks to top users, at least how many words the top user should send save_json: bool True if want to save results as json to exports folder Output json_results: list List of dictionaries in json format, ordered from predicted best to worst candidates. Ex: [{start:TIMESTAMP_INT, end:TIMESTAMP_INT}] thalamus ( big_df , min_ , goal , min_words ) Coordinates the other functions in this algo and data_helper. Separate from run() for sanity purposes.","title":"Algorithm 3.0"},{"location":"Algorithm_3.0/#prod.pillaralgos.algo3_0.id_words_counter","text":"Returns a dataframe with all user IDs and the number of words/emojis/combined they each sent, sorted by top senders","title":"id_words_counter()"},{"location":"Algorithm_3.0/#prod.pillaralgos.algo3_0.new_chunk_list","text":"Creates a new list of chunks, containing only chunks where top users sent more than min_words words","title":"new_chunk_list()"},{"location":"Algorithm_3.0/#prod.pillaralgos.algo3_0.results_formatter","text":"Creates a new df results that contains the total number of words in the dataframe, the time the time the dataframe started and ended","title":"results_formatter()"},{"location":"Algorithm_3.0/#prod.pillaralgos.algo3_0.results_formatter--input","text":"list_chunk: list List of pd.DataFrame goal: str Col name that has calculated results (ex: num_words, chat_rate, etc.)","title":"Input"},{"location":"Algorithm_3.0/#prod.pillaralgos.algo3_0.results_formatter--output","text":"results: pd.DataFrame Dataframe with `start`, `end`, `num_words` columns","title":"Output"},{"location":"Algorithm_3.0/#prod.pillaralgos.algo3_0.run","text":"Runs algo3_0 to extract only those chunks where the top 10 users participated. - Top users are defined as \"sent the most words in the entire twitch stream\". - Once top users are identified, only those chunks are returned where top users sent at least min_words number of words.","title":"run()"},{"location":"Algorithm_3.0/#prod.pillaralgos.algo3_0.run--input","text":"data: list List of dictionaries of data from Twitch chat min_: int Approximate number of minutes each clip should be limit: int Number of rows/dictionaries/timestamps to return min_words:int When filtering chunks to top users, at least how many words the top user should send save_json: bool True if want to save results as json to exports folder","title":"Input"},{"location":"Algorithm_3.0/#prod.pillaralgos.algo3_0.run--output","text":"json_results: list List of dictionaries in json format, ordered from predicted best to worst candidates. Ex: [{start:TIMESTAMP_INT, end:TIMESTAMP_INT}]","title":"Output"},{"location":"Algorithm_3.0/#prod.pillaralgos.algo3_0.thalamus","text":"Coordinates the other functions in this algo and data_helper. Separate from run() for sanity purposes.","title":"thalamus()"},{"location":"Algorithm_3.5/","text":"This script finds the number of words/emojis/both depending on goal variable, isolates to each min_ timestamp, and sorts the resulting df by largest number HOW TO algo3_5.run(data, min_=2, limit=10, goal='num_words_emo', save_json = False) num_words_in_chat ( dataframe ) Creates a new col num_words_emo that contains the number of words in each message results_formatter ( dataframe , goal ) Creates a new df results that contains the total number of words in the dataframe, the time the time the dataframe started and ended. Input dataframe: pd.DataFrame Dataframe with all the hours and chunks labeled, and num_words calculated goal: str Col name that has calculated results (ex: num_words, chat_rate, etc.) Output results: pd.DataFrame Dataframe with `hour`, `chunk`, `start`, `end`, `num_words` columns run ( data , min_ = 2 , limit = 10 , goal = 'num_words_emo' , save_json = False ) Runs algo3_5 to sort timestamps by the number of words+emojis by default. Input data: list List of dictionaries of data from Twitch chat min_: int Approximate number of minutes each clip should be limit: int Number of rows/dictionaries/timestamps to return goal: str 'num_words': sum of the number of words in each chat message 'num_emo': sum of the number of emoticons in each chat message 'num_words_emo': sum of the number of words + emoticons in each chat message save_json: bool True if want to save results as json to exports folder Output json_results: list List of dictionaries in json format, ordered from predicted best to worst candidates. Ex: [{start:TIMESTAMP_INT, end:TIMESTAMP_INT}] thalamus ( big_df , min_ , goal = 'num_words' ) Calculates num_words/emoji/both then runs through functions to split big_df, format for saving, and save as json Input big_df: pd.DataFrame twitch stream chat dataframe min_: int number of minutes each chunk should be goal: str one of `num_words`, `num_emo`, or `num_words_emo`","title":"Algorithm 3.5"},{"location":"Algorithm_3.5/#prod.pillaralgos.algo3_5.num_words_in_chat","text":"Creates a new col num_words_emo that contains the number of words in each message","title":"num_words_in_chat()"},{"location":"Algorithm_3.5/#prod.pillaralgos.algo3_5.results_formatter","text":"Creates a new df results that contains the total number of words in the dataframe, the time the time the dataframe started and ended.","title":"results_formatter()"},{"location":"Algorithm_3.5/#prod.pillaralgos.algo3_5.results_formatter--input","text":"dataframe: pd.DataFrame Dataframe with all the hours and chunks labeled, and num_words calculated goal: str Col name that has calculated results (ex: num_words, chat_rate, etc.)","title":"Input"},{"location":"Algorithm_3.5/#prod.pillaralgos.algo3_5.results_formatter--output","text":"results: pd.DataFrame Dataframe with `hour`, `chunk`, `start`, `end`, `num_words` columns","title":"Output"},{"location":"Algorithm_3.5/#prod.pillaralgos.algo3_5.run","text":"Runs algo3_5 to sort timestamps by the number of words+emojis by default.","title":"run()"},{"location":"Algorithm_3.5/#prod.pillaralgos.algo3_5.run--input","text":"data: list List of dictionaries of data from Twitch chat min_: int Approximate number of minutes each clip should be limit: int Number of rows/dictionaries/timestamps to return goal: str 'num_words': sum of the number of words in each chat message 'num_emo': sum of the number of emoticons in each chat message 'num_words_emo': sum of the number of words + emoticons in each chat message save_json: bool True if want to save results as json to exports folder","title":"Input"},{"location":"Algorithm_3.5/#prod.pillaralgos.algo3_5.run--output","text":"json_results: list List of dictionaries in json format, ordered from predicted best to worst candidates. Ex: [{start:TIMESTAMP_INT, end:TIMESTAMP_INT}]","title":"Output"},{"location":"Algorithm_3.5/#prod.pillaralgos.algo3_5.thalamus","text":"Calculates num_words/emoji/both then runs through functions to split big_df, format for saving, and save as json","title":"thalamus()"},{"location":"Algorithm_3.5/#prod.pillaralgos.algo3_5.thalamus--input","text":"big_df: pd.DataFrame twitch stream chat dataframe min_: int number of minutes each chunk should be goal: str one of `num_words`, `num_emo`, or `num_words_emo`","title":"Input"},{"location":"Brain/","text":"Coordinates all 4 algorithms, then compares across results to see if any timestamps are shared between them run ( data , clip_length , common_timestamps = 2 , algos_to_compare = [ 'algo1' , 'algo2' , 'algo3_0' , 'algo3_5' ], limit = None ) Coordinates all 4 algorithms, then compares across results to see if any timestamps are shared between them. Runs all algos on default param settings. Input data: list List of dictionaries, like a json file in the format [{'startTime':float, 'endTime':float}] common_timestamps: int Cutoff for how many algos should have a timestamp for it to be included in the results algos_to_compare: list List of one of: \"algo1\",\"algo2\",\"algo3_0\",\"algo3_5\" limit: int or None How many results should be returned. If None, all results surviving the X filter will be returned. Output new_json: list List of dictionaries, [{'startTime':float, 'endTime':float}]","title":"Brain"},{"location":"Brain/#prod.pillaralgos.brain.run","text":"Coordinates all 4 algorithms, then compares across results to see if any timestamps are shared between them. Runs all algos on default param settings.","title":"run()"},{"location":"Brain/#prod.pillaralgos.brain.run--input","text":"data: list List of dictionaries, like a json file in the format [{'startTime':float, 'endTime':float}] common_timestamps: int Cutoff for how many algos should have a timestamp for it to be included in the results algos_to_compare: list List of one of: \"algo1\",\"algo2\",\"algo3_0\",\"algo3_5\" limit: int or None How many results should be returned. If None, all results surviving the X filter will be returned.","title":"Input"},{"location":"Brain/#prod.pillaralgos.brain.run--output","text":"new_json: list List of dictionaries, [{'startTime':float, 'endTime':float}]","title":"Output"}]}