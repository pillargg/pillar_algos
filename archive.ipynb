{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where old code goes after upgraded to `helper` status. To keep algo notebooks free of clutter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: these were saved into `data_handler.py` helper library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_twitch_chat(data):\n",
    "    # all vars were loaded as str. Change type to datetime/int/bool\n",
    "    data['created_at'] = pd.to_datetime(data['created_at'])\n",
    "    data['updated_at'] = pd.to_datetime(data['updated_at'])\n",
    "    \n",
    "    df = data[['created_at','updated_at','commenter','message']]\n",
    "    \n",
    "    messages = df['message'].apply(pd.Series).drop(['fragments','user_color','user_notice_params'],axis=1)\n",
    "    users = df['commenter'].apply(pd.Series)\n",
    "    \n",
    "    df = df.drop(['message','commenter'], axis=1) # duplicate info\n",
    "    df = pd.concat([df,users,messages],axis=1)\n",
    "    df = df.iloc[:,[0,1,2,3,4,5,6,9,10,11,12,13]] # select cols that arent duplicates\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into hour sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dfSplitter():\n",
    "    def __init__(self, dataframe):\n",
    "        '''\n",
    "        Splits dataframe into multiple dataframes, each 1 hour long\n",
    "\n",
    "        output:\n",
    "        ------\n",
    "        my_list: list\n",
    "            List of dataframes\n",
    "        '''\n",
    "        # init function finds the first split\n",
    "        dataframe = dataframe.sort_values(\"created_at\")\n",
    "        first = dataframe[dataframe['created_at'] <= dataframe.loc[0,'created_at'] + pd.Timedelta(hours = 1)]\n",
    "        self.last_i = first.index.max()\n",
    "        self.dataframe = dataframe\n",
    "        self.result = [] # list to append starting timestamp + datasets to\n",
    "        self.result.append(dataframe.iloc[0, 0]) # NOTE: assumes first col is always \"created_at\" col\n",
    "        self.result.append(first)\n",
    "        \n",
    "    def find_rest(self):\n",
    "        '''\n",
    "        Uses last index of first split to find the others\n",
    "        '''\n",
    "        dataframe = self.dataframe\n",
    "        last_i = self.last_i\n",
    "        if last_i+1 != len(dataframe):\n",
    "            new_df = dataframe.loc[last_i+1:,:] # clip df to start at last_i\n",
    "            newest = new_df[new_df['created_at'] <= new_df.loc[last_i+1,'created_at'] + pd.Timedelta(hours=1)] # filter by hour\n",
    "            self.result.append(newest) # store in list\n",
    "            self.last_i = newest.index.max()\n",
    "            \n",
    "            self.find_rest() # repeat\n",
    "        else:\n",
    "            return dataframe # never actually used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split each section into X minute chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class xminChats():\n",
    "    def __init__(self,dataframe, big_unique, min_= 2):\n",
    "        '''\n",
    "        Finds the percent unique chatters that chatted every min_ minutes\n",
    "        \n",
    "        input\n",
    "        -----\n",
    "        dataframe: pd.DataFrame\n",
    "            Twitch chat dataframe organized and split by dfSplitter\n",
    "        big_unique: int\n",
    "            Total number of unique chatters for the entire Twitch stream\n",
    "        min_: int\n",
    "            Minute range to find timestamps for. Ex: Find 2 min long timestamps.\n",
    "        '''\n",
    "        \n",
    "        # init function finds the first split\n",
    "        dataframe = dataframe.sort_values(\"created_at\")\n",
    "        first = dataframe[dataframe['created_at'] <= dataframe.iloc[0,0] + pd.Timedelta(minutes = min_)]\n",
    "        \n",
    "        self.min_ = min_\n",
    "        self.total_uniques = len(dataframe['_id'].unique())\n",
    "        self.big_unique = big_unique\n",
    "        \n",
    "        self.last_i = first.index.max()\n",
    "        self.dataframe = dataframe\n",
    "        \n",
    "        self.result = []\n",
    "        self.result.append(first)\n",
    "        \n",
    "    def find_rest(self):\n",
    "        '''\n",
    "        Uses last index of first split to find the others\n",
    "        '''\n",
    "        dataframe = self.dataframe\n",
    "        last_i = self.last_i\n",
    "        if last_i+1 < dataframe.index.max(): # NOT len(dataframe), that bugs out and i dont wanna explain why\n",
    "            new_df = dataframe.loc[last_i+1:,:] # clip df to start new min_ min calc at last_i+1\n",
    "            newest = new_df[new_df['created_at'] <= new_df.loc[last_i+1,'created_at'] + pd.Timedelta(value=self.min_, unit='minutes')] # filter by minute\n",
    "            self.result.append(newest) # store in list\n",
    "            \n",
    "            self.last_i = newest.index.max()\n",
    "            self.find_rest() # repeat\n",
    "        else:\n",
    "            x=''\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format results as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_jsonified(results, first_sec, results_col):\n",
    "    '''\n",
    "    Converts timestamps to seconds, extracts results and makes the whole thing machine readable\n",
    "    \n",
    "    input\n",
    "    -----\n",
    "    results: pd.DataFrame\n",
    "        DataFrame with at least the start (datetime) and end (datetime) columns, and a column to sort by.\n",
    "    first_sec: datetime\n",
    "        The very first timestamp in the entire twitch chat log. Used to calculate elapsed time in seconds.        \n",
    "    results_col: str\n",
    "        Column to sort values by (ascending=False)\n",
    "        \n",
    "    output\n",
    "    ------\n",
    "    json_results: list\n",
    "        List of dictionaries with startTime and endTime keys, sorted by best results at top\n",
    "    '''\n",
    "    results['first_sec'] = first_sec # to calculate elapsed time from first sec, in seconds\n",
    "    results = results.sort_values(results_col, ascending=False) # so json format is returned with top result being the most relevant\n",
    "    json_results = []\n",
    "    for i, row in results.iterrows():\n",
    "        og = row['first_sec']\n",
    "        start = row['start']\n",
    "        end = row['end']\n",
    "        \n",
    "        start_sec = dt.timedelta.total_seconds(start-og) # find difference between first sec and given timestamp, convert that to seconds\n",
    "        end_sec = dt.timedelta.total_seconds(end-og)\n",
    "        \n",
    "        dict_ = {\"startTime\":start_sec,\n",
    "                 \"endTime\":end_sec}\n",
    "        json_results.append(dict_)\n",
    "        \n",
    "    return json_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(json_results, name):\n",
    "    '''\n",
    "    Saves json_results in txt file.\n",
    "    '''\n",
    "    str_  = '['\n",
    "    for dict_ in json_results:\n",
    "        str_ += str(dict_) + ', \\n '\n",
    "    str_ += ']'\n",
    "    \n",
    "    with open(f\"exports/{name}.json\",'w') as f:\n",
    "        f.write(str_)\n",
    "    print(f\"Saved to data/{name}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old emoticon getter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used to create the answer for unit test. Found a bug where means weren't being filtered with it.\n",
    "\n",
    "Keeping for posterity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 days 10:57:21.037000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-227a891fc555>:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  top_emoticons['vid_id'] = vid_id\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pillaralgos.helpers import data_handler as dh\n",
    "\n",
    "data = json.load(open(\"data/sample_lg.json\"))\n",
    "\n",
    "vid_id = data[0]['content_id']\n",
    "vid_id\n",
    "\n",
    "big_df = dh.organize_twitch_chat(data)\n",
    "print(big_df.iloc[-1,0] - big_df.iloc[0,0])\n",
    "\n",
    "def emo_extractor(my_list):\n",
    "    return [emo_dict['_id'] for emo_dict in my_list]\n",
    "\n",
    "\n",
    "def loc_extractor(my_list):\n",
    "    return [[emo_dict['begin'], emo_dict['end']] for emo_dict in my_list]\n",
    "\n",
    "all_emos = []\n",
    "def emo_saver(my_list):\n",
    "    for emo in my_list:\n",
    "        all_emos.append(emo)\n",
    "    return 'saved'\n",
    "\n",
    "emo_col = big_df['emoticons'].dropna().reset_index(drop=True)\n",
    "emo_id_list = emo_col.apply(lambda my_list: emo_extractor(my_list))\n",
    "emo_id_list.apply(lambda my_list: emo_saver(my_list))\n",
    "all_emos = pd.Series(all_emos)\n",
    "\n",
    "emo_loc = emo_col.apply(lambda my_list: loc_extractor(my_list))\n",
    "emo_body = big_df[~big_df['emoticons'].isna()]['body']\n",
    "\n",
    "emo_data = emo_loc.copy().reset_index()\n",
    "emo_data['body'] = emo_body.reset_index(drop=True)\n",
    "emo_data = emo_data.drop('index',axis=1)\n",
    "\n",
    "emo_data['id'] = emo_id_list\n",
    "\n",
    "emo_dict = {}\n",
    "\n",
    "for i, row in emo_data.iterrows():\n",
    "    for x in range(len(row['emoticons'])):\n",
    "        loc = row['emoticons'][x] # grab location\n",
    "        begin = loc[0]\n",
    "        end = loc[1] + 1\n",
    "        emoji_name = row['body'][begin:end] # extract emoji text\n",
    "        emoji_id = row['id'][x]\n",
    "        \n",
    "        if emoji_id not in emo_dict.keys():\n",
    "            emo_dict[emoji_id] = emoji_name\n",
    "\n",
    "num_used = all_emos.value_counts()\n",
    "\n",
    "num_used = num_used.reset_index()\n",
    "num_used.columns = ['emoji_id', 'occurrance']\n",
    "\n",
    "num_used['emoji_name'] = num_used['emoji_id'].map(emo_dict)\n",
    "\n",
    "num_used['label'] = ''\n",
    "top_emoticons = num_used[num_used['occurrance'] > num_used['occurrance'].mean()]\n",
    "top_emoticons['vid_id'] = vid_id\n",
    "top_emoticons = top_emoticons[['vid_id','emoji_id','occurrance','emoji_name','label']]\n",
    "\n",
    "# top_emoticons.to_csv(\"data/top_emoticons.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 55 entries, 0 to 54\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   vid_id      55 non-null     object\n",
      " 1   emoji_id    55 non-null     object\n",
      " 2   occurrance  55 non-null     int64 \n",
      " 3   emoji_name  55 non-null     object\n",
      " 4   label       55 non-null     object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 2.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 55 entries, 0 to 54\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   vid_id      55 non-null     object\n",
      " 1   emoji_id    55 non-null     object\n",
      " 2   occurrance  55 non-null     int64 \n",
      " 3   emoji_name  55 non-null     object\n",
      " 4   label       55 non-null     object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 2.6+ KB\n"
     ]
    }
   ],
   "source": [
    "top_emoticons.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(df == top_emoticons)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pillar Env",
   "language": "python",
   "name": "pillar_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
