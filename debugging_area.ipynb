{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pillaralgos.helpers import data_handler as d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is `data_handler.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This file contains a series of classes and functions to help with loading and splitting twitch chat data\n",
    "also json_saver() that converts given variable into string, saves into .json file\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "def rename_columns(col_string):\n",
    "    \"\"\"\n",
    "    Renames columns to be more presentable\n",
    "    \"\"\"\n",
    "    if (col_string == \"created_at_id\") | (col_string == \"updated_at_id\"):\n",
    "        col_string = col_string.replace(\"_id\", \"\")\n",
    "        return \"id_\" + col_string\n",
    "    elif (col_string == \"created_at_mess\") | (col_string == \"updated_at_mess\"):\n",
    "        col_string = col_string.replace(\"_mess\", \"\")\n",
    "        return col_string\n",
    "    else:\n",
    "        return col_string.replace(\"_mess\", \"\", 1).replace(\"_id\", \"\", 1)\n",
    "\n",
    "\n",
    "def select_columns(dataframe, keep_user_vars=False):\n",
    "    \"\"\"\n",
    "    Removes unneeded columns\n",
    "    \"\"\"\n",
    "    if keep_user_vars:\n",
    "        # If true, include user columns\n",
    "        bad_cols = [\"display_name_id\", \"name_id\", \"user_notice_params_mess\"]\n",
    "    else:\n",
    "        # If false, not analyzing the users\n",
    "        bad_cols = [\n",
    "            \"display_name_id\",\n",
    "            \"name_id\",\n",
    "            \"user_notice_params_mess\",\n",
    "            # these aren't used by the simpler fctns\n",
    "            \"bio_id\",\n",
    "            \"created_at_id\",\n",
    "            \"updated_at_id\",\n",
    "            \"logo_id\",\n",
    "        ]\n",
    "    dataframe = dataframe.drop(bad_cols, axis=1)\n",
    "    cols = dataframe.columns\n",
    "    cols = list(pd.Series(cols).apply(rename_columns))\n",
    "    dataframe.columns = cols\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def organize_twitch_chat(data, keep_user_vars=False):\n",
    "    \"\"\"\n",
    "    Turns json into dataframe. Expands lists of lists into own columns.\n",
    "\n",
    "    input\n",
    "    -----\n",
    "    data: list\n",
    "        list of dictionaries in json format, loaded with the `open` context manager.\n",
    "\n",
    "    output\n",
    "    ------\n",
    "    df: pd.DataFrame\n",
    "        Dataframe with the following columns:\n",
    "            ['created_at', 'updated_at', 'display_name', '_id', 'name', 'type',\n",
    "             'bio', 'logo', 'body', 'is_action', 'user_badges', 'emoticons']\n",
    "    \"\"\"\n",
    "    if len(data) > 0:\n",
    "        data = pd.DataFrame.from_records(data)  # convert to df\n",
    "        df = data[[\"created_at\", \"updated_at\", \"commenter\", \"message\"]].add_suffix(\"_mess\")\n",
    "\n",
    "        h = dictExtractor(df[\"message_mess\"], label=\"_mess\")\n",
    "        messages = h.result\n",
    "        g = dictExtractor(df[\"commenter_mess\"], label=\"_id\")\n",
    "        users = g.result\n",
    "\n",
    "        df = df.drop([\"message_mess\", \"commenter_mess\"], axis=1)  # duplicate info\n",
    "        df = pd.concat([df, users, messages], axis=1)\n",
    "        # all vars were loaded as str. Change type to datetime/int/bool\n",
    "        df = df.astype(\n",
    "            {\n",
    "                \"_id_id\": int,\n",
    "                \"bio_id\": \"category\",\n",
    "                \"created_at_id\": \"datetime64[ns]\",\n",
    "                \"created_at_mess\": \"datetime64[ns]\",\n",
    "                \"updated_at_id\": \"datetime64[ns]\",\n",
    "                \"updated_at_mess\": \"datetime64[ns]\",\n",
    "                \"is_action_mess\": bool,\n",
    "                \"type_id\": \"category\",\n",
    "            }\n",
    "        )\n",
    "        df = select_columns(df, keep_user_vars)\n",
    "        return df\n",
    "    else:\n",
    "        return np.array([])\n",
    "\n",
    "\n",
    "class dictExtractor:\n",
    "    def __init__(self, my_series, label=\"\"):\n",
    "        \"\"\"\n",
    "        Extracts dictionaries from series into a new dict using the\n",
    "        longest dictionary's keys. Converts new dict into df, stored\n",
    "        as `self.result`. Because not every dict had same number of keys.\n",
    "\n",
    "        input\n",
    "        -----\n",
    "        my_series: pd.Series\n",
    "            A column from twitch dataframe where each row is a dict\n",
    "        label: str\n",
    "            What will be appended to the end of each col\n",
    "        \"\"\"\n",
    "        # find max length of dicts\n",
    "        length = my_series.apply(lambda x: len(x))\n",
    "        y = 0\n",
    "        for x in length:\n",
    "            if x > y:\n",
    "                y = x\n",
    "        # find index of max keys dict\n",
    "        ind = length[length == y].index[0]\n",
    "        max_d = my_series.iloc[ind].keys()\n",
    "        self.max_d = max_d\n",
    "        # initiate new dict\n",
    "        self.new_dict = {}\n",
    "        for k in max_d:\n",
    "            self.new_dict[k] = []\n",
    "        # extract dict values into new dict\n",
    "        my_series.apply(lambda x: self.keys_iterator(x))\n",
    "        # store as df\n",
    "        self.result = pd.DataFrame.from_dict(self.new_dict)\n",
    "        # df.add_suffix() is actually 0.25 seconds slower\n",
    "        self.result.columns = [col + label for col in self.result.columns]\n",
    "\n",
    "    def keys_iterator(self, my_dict):\n",
    "        \"\"\"\n",
    "        Checks that all of the `max_d` are in the given dictionary. If not,\n",
    "        appends np.nan. Otherwise appends the value.\n",
    "        \"\"\"\n",
    "        for k in self.max_d:\n",
    "            if k not in my_dict.keys():\n",
    "                self.new_dict[k].append(np.nan)\n",
    "            else:\n",
    "                self.new_dict[k].append(my_dict[k])\n",
    "\n",
    "\n",
    "class dfSplitter:\n",
    "    def __init__(self, dataframe):\n",
    "        \"\"\"\n",
    "        Splits dataframe into multiple dataframes, each 1 hour long\n",
    "\n",
    "        output:\n",
    "        ------\n",
    "        my_list: list\n",
    "            List of dataframes\n",
    "        \"\"\"\n",
    "        # init function finds the first split\n",
    "        dataframe = dataframe.sort_values(\"created_at\")\n",
    "        first = dataframe[\n",
    "            dataframe[\"created_at\"]\n",
    "            <= dataframe.loc[0, \"created_at\"] + pd.Timedelta(hours=1)\n",
    "        ]\n",
    "        self.last_i = first.index.max()\n",
    "        self.dataframe = dataframe\n",
    "        self.result = []  # list to append starting timestamp + datasets to\n",
    "        self.result.append(\n",
    "            dataframe.iloc[0, 0]\n",
    "        )  # NOTE: assumes first col is always \"created_at\" col\n",
    "        self.result.append(first)\n",
    "\n",
    "    def find_rest(self):\n",
    "        \"\"\"\n",
    "        Uses last index of first split to find the others\n",
    "        \"\"\"\n",
    "        dataframe = self.dataframe\n",
    "        last_i = self.last_i\n",
    "        if last_i + 1 != len(dataframe):\n",
    "            new_df = dataframe.loc[last_i + 1 :, :]  # clip df to start at last_i\n",
    "            newest = new_df[\n",
    "                new_df[\"created_at\"]\n",
    "                <= new_df.loc[last_i + 1, \"created_at\"] + pd.Timedelta(hours=1)\n",
    "            ]  # filter by hour\n",
    "            self.result.append(newest)  # store in list\n",
    "            self.last_i = newest.index.max()\n",
    "\n",
    "            self.find_rest()  # repeat\n",
    "        else:\n",
    "            return dataframe  # never actually used\n",
    "\n",
    "\n",
    "class xminChats:\n",
    "    def __init__(self, dataframe, big_unique, min_=2):\n",
    "        \"\"\"\n",
    "        Finds the percent unique chatters that chatted every min_ minutes\n",
    "\n",
    "        input\n",
    "        -----\n",
    "        dataframe: pd.DataFrame\n",
    "            Twitch chat dataframe organized and split by dfSplitter\n",
    "        big_unique: int\n",
    "            Total number of unique chatters for the entire Twitch stream\n",
    "        min_: int\n",
    "            Minute range to find timestamps for. Ex: Find 2 min long timestamps.\n",
    "        \"\"\"\n",
    "\n",
    "        # init function finds the first split\n",
    "        dataframe = dataframe.sort_values(\"created_at\")\n",
    "        first = dataframe[\n",
    "            dataframe[\"created_at\"] <= dataframe.iloc[0, 0] + pd.Timedelta(minutes=min_)\n",
    "        ]\n",
    "\n",
    "        self.min_ = min_\n",
    "        self.total_uniques = len(dataframe[\"_id\"].unique())\n",
    "        self.big_unique = big_unique\n",
    "\n",
    "        self.last_i = first.index.max()\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "        self.result = []\n",
    "        self.result.append(first)\n",
    "\n",
    "    def find_rest(self):\n",
    "        \"\"\"\n",
    "        Uses last index of first split to find the others\n",
    "        \"\"\"\n",
    "        dataframe = self.dataframe\n",
    "        last_i = self.last_i\n",
    "        if (\n",
    "            last_i + 1 < dataframe.index.max()\n",
    "        ):  # NOT len(dataframe), that bugs out and i dont wanna explain why\n",
    "            new_df = dataframe.loc[\n",
    "                last_i + 1 :, :\n",
    "            ]  # clip df to start new min_ min calc at last_i+1\n",
    "            newest = new_df[\n",
    "                new_df[\"created_at\"]\n",
    "                <= new_df.loc[last_i + 1, \"created_at\"]\n",
    "                + pd.Timedelta(value=self.min_, unit=\"minutes\")\n",
    "            ]  # filter by minute\n",
    "            self.result.append(newest)  # store in list\n",
    "\n",
    "            self.last_i = newest.index.max()\n",
    "            self.find_rest()  # repeat\n",
    "        else:\n",
    "            x = \"\"\n",
    "\n",
    "\n",
    "def get_chunks(dataframe, min_=2):\n",
    "    \"\"\"\n",
    "    Iterates through the data_helper classes to divide dataframe into chunks\n",
    "\n",
    "    input\n",
    "    -----\n",
    "    dataframe: pd.DataFrame\n",
    "        The entire twitch stream chat df\n",
    "    min_: int\n",
    "        The min_ value to pass into xminChats()\n",
    "\n",
    "    output\n",
    "    ------\n",
    "    first_stamp: datetime\n",
    "        The very first timestamp of dataframe\n",
    "    chunk_list:\n",
    "        List of `min_` long dataframes\n",
    "    \"\"\"\n",
    "    dhs = dfSplitter(dataframe)\n",
    "    dhs.find_rest()\n",
    "    hour_list = dhs.result\n",
    "\n",
    "    first_stamp = hour_list[0]\n",
    "    del hour_list[0]\n",
    "\n",
    "    chunk_list = []\n",
    "    for i in range(len(hour_list)):\n",
    "        hour = hour_list[i]\n",
    "\n",
    "        dhx = xminChats(hour, dataframe[\"_id\"].unique(), min_=min_)\n",
    "        dhx.find_rest()\n",
    "        chunks = dhx.result\n",
    "\n",
    "        for x in range(len(chunks)):\n",
    "            chunk = chunks[x]\n",
    "            chunk[\"hour\"] = i\n",
    "            chunk[\"chunk\"] = x\n",
    "            chunk_list.append(chunk)\n",
    "    return first_stamp, chunk_list\n",
    "\n",
    "\n",
    "def results_jsonified(results, first_sec, results_col):\n",
    "    \"\"\"\n",
    "    Converts timestamps to seconds, extracts results and makes the whole thing machine readable\n",
    "\n",
    "    input\n",
    "    -----\n",
    "    results: pd.DataFrame\n",
    "        DataFrame with at least the start (datetime) and end (datetime) columns, and a column to sort by.\n",
    "    first_sec: datetime\n",
    "        The very first timestamp in the entire twitch chat log. Used to calculate elapsed time in seconds.\n",
    "    results_col: str\n",
    "        Column to sort values by (ascending=False)\n",
    "\n",
    "    output\n",
    "    ------\n",
    "    json_results: list\n",
    "        List of dictionaries with startTime and endTime keys, sorted by best results at top\n",
    "    \"\"\"\n",
    "    results[\n",
    "        \"first_sec\"\n",
    "    ] = first_sec  # to calculate elapsed time from first sec, in seconds\n",
    "    results = results.sort_values(\n",
    "        results_col, ascending=False\n",
    "    )  # so json format is returned with top result being the most relevant\n",
    "    json_results = []\n",
    "    for i, row in results.iterrows():\n",
    "        og = row[\"first_sec\"]\n",
    "        start = row[\"start\"]\n",
    "        end = row[\"end\"]\n",
    "\n",
    "        start_sec = dt.timedelta.total_seconds(\n",
    "            start - og\n",
    "        )  # find difference between first sec and given timestamp, convert that to seconds\n",
    "        end_sec = dt.timedelta.total_seconds(end - og)\n",
    "\n",
    "        dict_ = {\"startTime\": start_sec, \"endTime\": end_sec}\n",
    "        json_results.append(dict_)\n",
    "\n",
    "    return json_results\n",
    "\n",
    "\n",
    "def save_json(json_results, name):\n",
    "    \"\"\"\n",
    "    Converts list of dict to pure str, then saves as a json file.\n",
    "\n",
    "    input\n",
    "    -----\n",
    "    json_results: list\n",
    "        List of dictionaries containing results\n",
    "    name: str\n",
    "        Filename (with optional directory) to save as. Ex: name.json or exports/name.json\n",
    "    \"\"\"\n",
    "    str_ = \"[\"\n",
    "    for dict_ in json_results:\n",
    "        str_ += str(dict_) + \", \\n \"\n",
    "    str_ += \"]\"\n",
    "\n",
    "    with open(f\"{name}.json\", \"w\") as f:\n",
    "        f.write(str_)\n",
    "    print(f\"Saved to {name}.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is `algo1.py`, needed it to be modified (line `140`) to return nonjson results for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sorts the final results by `perc_rel_unique`. Calculated as \"number of chatters\n",
    "at timestamp\"/\"number of chatters in that one hour\"\n",
    "\n",
    "HOW TO\n",
    "    algo1.run(data, min_=2, limit=10, sort_by='rel', save_json = False)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def perc_uniques(chunk_list, min_, total_uniques, big_unique):\n",
    "    \"\"\"\n",
    "    Finds the percent unique chatters for each dataframe in the list. Dataframes\n",
    "    assumed to be split using xminChats.find_rest.\n",
    "    \"\"\"\n",
    "\n",
    "    perc_unique = {\n",
    "        f\"{min_}min_chunk\": [],\n",
    "        \"start\": [],\n",
    "        \"end\": [],\n",
    "        \"num_unique\": [],\n",
    "        \"perc_rel_unique\": [],\n",
    "        \"perc_abs_unique\": [],\n",
    "    }\n",
    "\n",
    "    for i in range(len(chunk_list)):\n",
    "        # calcuate\n",
    "        chunk = i\n",
    "        unique = len(chunk_list[i][\"_id\"].unique())\n",
    "        timestamp = [\n",
    "            chunk_list[i][\"created_at\"].min(),\n",
    "            chunk_list[i][\"created_at\"].max(),\n",
    "        ]\n",
    "        perc_rel = (\n",
    "            unique / total_uniques\n",
    "        )  # this is the total uniques in THAT DATAFRAME, ie the hourly cut\n",
    "        perc_abs = (\n",
    "            unique / big_unique\n",
    "        )  # this is the total uniques in the entire twitch session\n",
    "        # store\n",
    "        perc_unique[f\"{min_}min_chunk\"].append(chunk)\n",
    "        perc_unique[\"start\"].append(timestamp[0])\n",
    "        perc_unique[\"end\"].append(timestamp[1])\n",
    "        perc_unique[\"num_unique\"].append(unique)\n",
    "        perc_unique[\"perc_rel_unique\"].append(perc_rel)\n",
    "        perc_unique[\"perc_abs_unique\"].append(perc_abs)\n",
    "\n",
    "    df_unique = pd.DataFrame(perc_unique)\n",
    "    df_unique[\"elapsed\"] = df_unique[\"end\"] - df_unique[\"start\"]\n",
    "    return df_unique\n",
    "\n",
    "\n",
    "def hour_iterator(big_df, limit, min_=2, sort_by=\"rel\"):\n",
    "    \"\"\"\n",
    "    Pushes all dfs in a list through the xminChats function, returns a dataframe of results\n",
    "\n",
    "    input\n",
    "    -----\n",
    "    big_df: pd.DataFrame\n",
    "        Df of the entire twitch session. This is the one that was split by dfSplitter class\n",
    "    min_: int\n",
    "        How long a timestamp range should be\n",
    "    sort_by: str\n",
    "        Whether to sort values by `abs` or `rel` unique chatters.\n",
    "    \"\"\"\n",
    "    ds = d.dfSplitter(big_df)  # initiate\n",
    "    ds.find_rest()  # split big_df into 1 hour long separate dfs\n",
    "    hour_list = (\n",
    "        ds.result\n",
    "    )  # result stored in class var. NOTE: index 0 is always the very first timestamp of big_df\n",
    "    first_sec = hour_list[0]\n",
    "    hour_list = hour_list[1:]\n",
    "\n",
    "    # initiate empty results df\n",
    "    results = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"hour\",\n",
    "            f\"{min_}min_chunk\",\n",
    "            \"start\",\n",
    "            \"end\",\n",
    "            \"num_unique\",\n",
    "            \"perc_rel_unique\",\n",
    "            \"perc_abs_unique\",\n",
    "        ]\n",
    "    )\n",
    "    max_uniques = len(\n",
    "        big_df[\"_id\"].unique()\n",
    "    )  # the total number of unique chatters for the entire twitch session\n",
    "\n",
    "    # iterate all sections through the class\n",
    "    for i in range(len(hour_list)):\n",
    "        fm = d.xminChats(hour_list[i], max_uniques, min_=min_)\n",
    "        _n = fm.find_rest()  # _n not needed\n",
    "        chunk_list = fm.result  # get back list of dfs, each 2 minutes long\n",
    "\n",
    "        hr_uniques = perc_uniques(\n",
    "            chunk_list, min_, total_uniques=fm.total_uniques, big_unique=fm.big_unique\n",
    "        )\n",
    "        hr_uniques[\"hour\"] = i + 1\n",
    "        results = results.append(hr_uniques)\n",
    "\n",
    "    results[\"elapsed\"] = results[\"end\"] - results[\"start\"]  # to double check length\n",
    "    pretty_results = results.reset_index(drop=True)  # prettify\n",
    "    pretty_results = pretty_results.sort_values(\n",
    "        f\"perc_{sort_by}_unique\", ascending=False\n",
    "    )\n",
    "    results = results.head(10)\n",
    "\n",
    "    json_results = d.results_jsonified(\n",
    "        results, first_sec, results_col=f\"perc_{sort_by}_unique\"\n",
    "    )  # ordered by top perc_rel_unique\n",
    "\n",
    "    return pretty_results, json_results\n",
    "\n",
    "\n",
    "def run(data, min_=2, limit=10, sort_by=\"rel\", save_json=False):\n",
    "    \"\"\"\n",
    "    Runs algo1 to sort timestamps by the relative percentage of chatters by default.\n",
    "\n",
    "    input:\n",
    "    ------\n",
    "    data: list\n",
    "        List of dictionaries of data from Twitch chat\n",
    "    min_: int\n",
    "        Approximate number of minutes each clip should be\n",
    "    limit: int\n",
    "        Number of rows/dictionaries/timestamps to return\n",
    "    sort_by: str\n",
    "        'rel': \"number of chatters at timestamp\"/\"number of chatters at that hour\"\n",
    "        'abs': \"number of chatters at timestamp\"/\"total number of chatters in stream\"\n",
    "    save_json: bool\n",
    "        True if want to save results as json to exports folder\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame.from_records(data)\n",
    "    big_df = d.organize_twitch_chat(data)  # fetch appropriate data\n",
    "    if type(big_df) == pd.DataFrame:\n",
    "        results, json_results = hour_iterator(big_df, limit=limit, min_=min_, sort_by=sort_by)\n",
    "        if save_json:\n",
    "            d.save_json(json_results, name=f\"algo1_perc_{sort_by}_unique\")\n",
    "        return results # json_results\n",
    "    else:\n",
    "        return big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('data/sample_med.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'pypi/prod/test/sample_data'\n",
    "# df = organize_twitch_chat(data)\n",
    "# df.to_csv(f'{test_dir}/sample_med_organized.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = run(data)\n",
    "results_df = results_df.sort_values('perc_rel_unique', ascending=False)\n",
    "results_df = results_df[['start','end','perc_rel_unique']]\n",
    "results_col = 'perc_rel_unique'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sec = results_df.loc[0,'start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(f\"{test_dir}/sample_med_resultsdf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = pd.read_csv(f\"{test_dir}/sample_med_resultsdf.csv\")\n",
    "reloaded = reloaded\n",
    "# reloaded = reloaded[[\"start\", \"end\", \"perc_rel_unique\"]]\n",
    "reloaded = reloaded.astype(\n",
    "    {\"start\": \"datetime64[ns]\",\n",
    "     \"end\": \"datetime64[ns]\",\n",
    "     \"perc_rel_unique\": float}\n",
    ")\n",
    "reloaded_sec = reloaded.sort_values('Unnamed: 0').iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_sec == first_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 212 entries, 210 to 175\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   start            212 non-null    datetime64[ns]\n",
      " 1   end              212 non-null    datetime64[ns]\n",
      " 2   perc_rel_unique  212 non-null    float64       \n",
      "dtypes: datetime64[ns](2), float64(1)\n",
      "memory usage: 14.7 KB\n"
     ]
    }
   ],
   "source": [
    "results_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 212 entries, 0 to 211\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   Unnamed: 0       212 non-null    int64         \n",
      " 1   start            212 non-null    datetime64[ns]\n",
      " 2   end              212 non-null    datetime64[ns]\n",
      " 3   perc_rel_unique  212 non-null    float64       \n",
      "dtypes: datetime64[ns](2), float64(1), int64(1)\n",
      "memory usage: 6.8 KB\n"
     ]
    }
   ],
   "source": [
    "reloaded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_j = results_jsonified(reloaded, reloaded_sec, results_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_j = results_jsonified(results_df, first_sec, results_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'startTime': 25874.858, 'endTime': 25994.723},\n",
       " {'startTime': 25630.013, 'endTime': 25749.664},\n",
       " {'startTime': 25753.034, 'endTime': 25870.105},\n",
       " {'startTime': 12177.737, 'endTime': 12293.058},\n",
       " {'startTime': 26005.689, 'endTime': 26046.392}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_j[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'startTime': 25874.858, 'endTime': 25994.723},\n",
       " {'startTime': 25630.013, 'endTime': 25749.664},\n",
       " {'startTime': 25753.034, 'endTime': 25870.105},\n",
       " {'startTime': 12177.737, 'endTime': 12293.058},\n",
       " {'startTime': 26005.689, 'endTime': 26046.392}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_j_noi = results_jsonified(results_df.reset_index(drop=True), first_sec, results_col)\n",
    "results_j_noi[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_j_noi == results_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_j) == len(results_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "failures = []\n",
    "for i in range(len(new_j)):\n",
    "    if new_j[i] == results_j[i]:\n",
    "        counter+=1\n",
    "    else:\n",
    "        failures.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = [{'startTime': 18725.355, 'endTime': 18845.192},\n",
    " {'startTime': 24129.932, 'endTime': 24249.818},\n",
    " {'startTime': 12012.096, 'endTime': 12131.918},\n",
    " {'startTime': 34939.788, 'endTime': 35058.316},\n",
    " {'startTime': 27129.808, 'endTime': 27249.795}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'startTime': 25874.858, 'endTime': 25994.723},\n",
       " {'startTime': 25630.013, 'endTime': 25749.664},\n",
       " {'startTime': 25753.034, 'endTime': 25870.105},\n",
       " {'startTime': 12177.737, 'endTime': 12293.058},\n",
       " {'startTime': 26005.689, 'endTime': 26046.392}]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_j[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pillar Env",
   "language": "python",
   "name": "pillar_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
